---
title: "Benchmark forecasting of Uber"
output: html_notebook
---

```{r}
source('uber_data_preprocessing.R')
source('error_metrics.R')
```

First, I will investigate the structure of the uber data set by utilizing the four_plot function.

```{r}
four_plot(uber_train)
```
As seen in the top-left plot, the data shows a considerable trend. The location of the series changes over time. However, it seems that the variation of the series is almost constant (I will check this quantitavely). As a result, I believe no Boxcox transformation is necessary for this series. 
```{r}
# exploring if a BoxCox transformation is necesary or not
print(BoxCox.lambda(uber_train))
```
The suggested lambda value for the BoxCox transformation in 1.19. This value is very close to 1 (i.e. the original time series). As a result, I decide not to apply any tranformation on this time series.


The top-right plot is the lag plot. The significant linear structure in this plot, demonstrates that 1) the series is not in a random manner, 2) there is a strong aurocorrelation in the series, 3) no obvious outliers exist in the data.

The bottom-left plot, shows the distribution of the data. Since the data is not random, this histogram does not provide much information. However, the histogram can confirm that no significant outliers exist in the data.

The bottom-right plot is the ACF plot. As seen, there is a strong auto-correlation within the data. Such auto-correlation typically represent a random walk series with drift. This should be investigated further. The ACF plot does not show a strong seasonality within the data. Therefore, I will quantitavely explore the seasonality.

```{r}
# further investigation on the autocorrelation
ggPacf(uber_train)

```
The PACF plot shows considerable spikes at lags 1, 2, and 3. As a result, since there are spikes on lags 2 and 3, the random walk model with the drift might not be the most appropriate model. (although it may work well)

```{r}
# further investigation of the trend and seasonality
print(trend_seasonality_strength(uber_train))
ggseasonplot(uber_train)
ggsubseriesplot(uber_train)
```
The trend_seasonality_strength function shows that the trend is significant. Also, intrestingly, it shows that there is a weak to medium seasonality in the data. By looking at the seasonal plots, it could be understood that during summer months, specifically in last three years, there is a peak (albeit, not a significant one). Therefore, I believe that benchmark models (including naive and random walk with drift) may not be the best choices and more sophisticated models are required. However, I am going to do the forecast with the benchmark models, calculate their performance, compare their performance, and demonstrate that they are not the best choice.

The models that I am going to explore are:
1) average model
2) naive model (no drift)
3) random walk with drift
4) seasonal naive

```{r}
# defining the length of the forecast horizon
h = length(uber_test)

# fitting models to the train data
average_model <- meanf(y=uber_train, h=h)
naive_model <- naive(y=uber_train, h=h)
rwdrift <- rwf(y=uber_train, h=h, drift = T)
seasonal_naive <- snaive(y=uber_train, h=h)

# showing how models fit the data
autoplot(uber_train, series = 'training')+
  autolayer(fitted(average_model), series='average model')+
  autolayer(fitted(naive_model), series='naive model')+
  autolayer(fitted(rwdrift), series='rw drift')+
  autolayer(fitted(seasonal_naive), series='seasonal naive')
```
As expected, the average model and seasonal naive models are not performing very well. The naive and random walk with drift models seem to perform very similar. Now, lets take a look at their performance in a quantitative manner.

```{r}
print('average model performance')
accuracy(average_model)
print('naive model performance')
accuracy(naive_model)
print('random walk with drift model performance')
accuracy(rwdrift)
print('seasonal naive model performance')
accuracy(seasonal_naive)

```
As seen, the error metrics for average and seasonal naive models are very poor. For naive and random walk with drift, the erros are very similar and not very bad. In order or see which model outperforms the other, I am going to use the cross validation only for our top two choices.

```{r}
naive_error <- tsCV(y=uber_train, forecastfunction = naive, h=1)
rwdrift_error <- tsCV(y=uber_train, forecastfunction = rwf, drift=T, h=1)
print(rmse(naive_error))
print(rmse(rwdrift_error))
```
Based on the RMSE value on rolling based cross validation, it seems that the naive method is slightly better thatn the random walk with drift. Therefore, I will select the naive model as the best benchmark model for this time series. Now, let's take a look at its residuals.
```{r}
# exploring the residuals of the naive model
residual_diognostics_plots(naive_model)
```
According to the residual diognastics plots, the naive model may not be a very good model. There are several reasons for that as follows:
1- The residuals do not follow a random distribution, suggesting that the confidence intervals of the forecast could not be very accurate.

2- Residuals versus fitted values shows some structure, as the fitted value increases the residuals variation increases. 

3- The ACF and PACF plots demonstrate significant spikes for at leats two lag values. This means that the time dep[endency that exits in the data is not explained properly by the naive model

As a result, I believe more sophisticated models should be investigated. However, lets do the forecast for the year 2018 using the naive model and see how it performs.



```{r}
naive_fcst <- forecast(naive_model, h=h)
autoplot(uber) +
  autolayer(naive_fcst, alpha=0.5)

```

```{r}
# looking at the error metrics for both training and testing sets
print(accuracy(naive_model, uber))

```
As seen, the performance on the testing set is slithly worse than the training set. 
As I mentioned, I believe this model does not capture all time-dependency pattersn in the data. Therefore, more sophisticated models are required. In the next notebook, I will investigate linear models. Stay tuned. 

